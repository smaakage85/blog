<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pRopaganda by smaakagen</title>
    <link>/</link>
    <description>Recent content on pRopaganda by smaakagen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 21 Dec 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>&#39;dockr&#39;: easy containerization for R</title>
      <link>/2019/12/21/dockr-easy-containerization-for-r/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/21/dockr-easy-containerization-for-r/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/smaakage85/dockr/blob/master/man/figures/dockr.png?raw=true&#34; align=&#34;right&#34; height=250/&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dockr&lt;/code&gt; 0.8.6 is now available on CRAN. &lt;code&gt;dockr&lt;/code&gt; is a minimal toolkit to build a
lightweight Docker container image for your R package, in which the package
itself is available. The Docker image seeks to mirror your R session as close as
possible with respect to R specific dependencies. Both dependencies on CRAN
R packages as well as local non-CRAN R packages will be included in the Docker
container image.&lt;/p&gt;
&lt;p&gt;If you want to know, how Docker works, and why you should consider using Docker,
please take a look at the &lt;a href=&#34;https://www.docker.com/why-docker&#34;&gt;Docker website&lt;/a&gt;.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/7LL40F6YdZgeiQ6en1c7Lk&#34; width=&#34;300&#34; height=&#34;80&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;Install the development version of &lt;code&gt;dockr&lt;/code&gt; with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remotes::install_github(&amp;quot;smaakage85/dockr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or install the version released on CRAN:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;dockr&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Workflow&lt;/h2&gt;
&lt;p&gt;When you work on an R project, it is often desirable to organize the code in the
R package structure. &lt;code&gt;dockr&lt;/code&gt; facilitates easy creation of a Docker container
image that mirrors your current R session and includes all of the R dependencies
needed to run your R package.&lt;/p&gt;
&lt;p&gt;First, load the &lt;code&gt;dockr&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dockr)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order do create the files, that constitute the Docker image, simply invoke
the &lt;code&gt;prepare_docker_image()&lt;/code&gt; function and point to the folder with your package.&lt;/p&gt;
&lt;p&gt;The workflow of &lt;code&gt;prepare_docker_image()&lt;/code&gt; is summarized below:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Build and install the package on your system&lt;/li&gt;
&lt;li&gt;Identify R package dependencies of the package&lt;/li&gt;
&lt;li&gt;Detect the version numbers of the loaded and installed versions of these
packages on your system&lt;/li&gt;
&lt;li&gt;Write Dockerfile and create all other files needed to build the Docker image&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, I will let &lt;code&gt;dockr&lt;/code&gt; do its magic and create the files for a Docker image
container, in which &lt;code&gt;dockr&lt;/code&gt; is installed together with all of the R package
dependencies, &lt;code&gt;dockr&lt;/code&gt; needs to run.&lt;/p&gt;
&lt;p&gt;Beware that the files are created as side-effects of the function call. Since
my ‘dockr’ package lives in a folder called ‘docker’ misleadingly, I call the
function like this:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;image_dockr &amp;lt;- prepare_docker_image(&amp;quot;~/docker&amp;quot;, 
                                    dir_image = &amp;quot;~&amp;quot;,
                                    dir_install = &amp;quot;auto&amp;quot;)
#&amp;gt; v Deleting existing folder for files for Docker image: ~/dockr_0.8.6
#&amp;gt; v Creating folder for files for Docker image: ~/dockr_0.8.6
#&amp;gt; v Creating folder for source packages: ~/dockr_0.8.6/source_packages
#&amp;gt; v Creating empty Dockerfile: ~/dockr_0.8.6/Dockerfile
#&amp;gt; --- Building, installing and loading package...
#&amp;gt; Writing NAMESPACE
#&amp;gt; Writing NAMESPACE
#&amp;gt; --- Writing Dockerfile...
#&amp;gt; v Preparing FROM statement
#&amp;gt; v Identifying and mirroring R package dependencies
#&amp;gt; v Matching dependencies with CRAN packages
#&amp;gt; v Preparing install statements for specific versions of CRAN packages
#&amp;gt; v Preparing install statement for the package itself
#&amp;gt; v Writing lines to Dockerfile
#&amp;gt; v Closing connection to Dockerfile
#&amp;gt; - in R : 
#&amp;gt; =&amp;gt; to inspect Dockerfile run:
#&amp;gt; dockr::print_file(&amp;quot;~/dockr_0.8.6/Dockerfile&amp;quot;) 
#&amp;gt; =&amp;gt; to edit Dockerfile run:
#&amp;gt; dockr::write_lines_to_file([lines], &amp;quot;~/dockr_0.8.6/Dockerfile&amp;quot;) 
#&amp;gt; - in Shell : 
#&amp;gt; =&amp;gt; to build Docker image run:
#&amp;gt; cd C:\Users\Lars\Documents\dockr_0.8.6 
#&amp;gt; docker build -t dockr_0.8.6 . 
#&amp;gt; Please note that Docker must be installed in order for you to build image.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, argument ‘dir_image’ decides, where the files for the docker image will
be saved. ‘dir_install’ is the directory, where your package will be installed
on your system. You can choose to install the package in a temporary folder
by setting &lt;code&gt;dir_install = tempdir()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Great, all necessary files for the Docker image have been created, and you
can build the Docker image right away by following the instructions. It is as
easy as that! Yeah!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;files-for-docker-image&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Files for Docker image&lt;/h2&gt;
&lt;p&gt;Let us just take a quick look into the folder with the files for the Docker
image to see the works of &lt;code&gt;dockr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list.files(image_dockr$paths$dir_image)
#&amp;gt; [1] &amp;quot;Dockerfile&amp;quot;      &amp;quot;source_packages&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It contains a Dockerfile and a folder named ‘source_packages’.&lt;/p&gt;
&lt;div id=&#34;dockerfile&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dockerfile&lt;/h3&gt;
&lt;p&gt;The resulting Dockerfile can be printed with the &lt;code&gt;print_file()&lt;/code&gt; function, that
comes with &lt;code&gt;dockr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_file(image_dockr$paths$path_Dockerfile)
#&amp;gt; # load rocker base-R image
#&amp;gt; FROM rocker/r-ver:3.6.0
#&amp;gt; 
#&amp;gt; # install specific versions of CRAN packages from MRAN snapshots
#&amp;gt; RUN R -e &amp;#39;install.packages(&amp;quot;remotes&amp;quot;)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;askpass&amp;quot;, &amp;quot;1.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;assertthat&amp;quot;, &amp;quot;0.2.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;brew&amp;quot;, &amp;quot;1.0-6&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;clisymbols&amp;quot;, &amp;quot;1.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;commonmark&amp;quot;, &amp;quot;1.7&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;crayon&amp;quot;, &amp;quot;1.3.4&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;desc&amp;quot;, &amp;quot;1.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;fs&amp;quot;, &amp;quot;1.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;gh&amp;quot;, &amp;quot;1.0.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;glue&amp;quot;, &amp;quot;1.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;gtools&amp;quot;, &amp;quot;3.8.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;ini&amp;quot;, &amp;quot;0.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;jsonlite&amp;quot;, &amp;quot;1.6&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;magrittr&amp;quot;, &amp;quot;1.5&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;memoise&amp;quot;, &amp;quot;1.1.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;pkgload&amp;quot;, &amp;quot;1.0.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;prettyunits&amp;quot;, &amp;quot;1.0.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;ps&amp;quot;, &amp;quot;1.3.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rcmdcheck&amp;quot;, &amp;quot;1.3.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rprojroot&amp;quot;, &amp;quot;1.3-2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rstudioapi&amp;quot;, &amp;quot;0.10&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;sessioninfo&amp;quot;, &amp;quot;1.1.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;stringi&amp;quot;, &amp;quot;1.4.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;stringr&amp;quot;, &amp;quot;1.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;withr&amp;quot;, &amp;quot;2.1.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;xopen&amp;quot;, &amp;quot;1.0.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;yaml&amp;quot;, &amp;quot;2.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;backports&amp;quot;, &amp;quot;1.1.4&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;callr&amp;quot;, &amp;quot;3.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;cli&amp;quot;, &amp;quot;1.1.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;clipr&amp;quot;, &amp;quot;0.6.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;curl&amp;quot;, &amp;quot;3.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;devtools&amp;quot;, &amp;quot;2.0.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;digest&amp;quot;, &amp;quot;0.6.20&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;git2r&amp;quot;, &amp;quot;0.25.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;httr&amp;quot;, &amp;quot;1.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;mime&amp;quot;, &amp;quot;0.6&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;openssl&amp;quot;, &amp;quot;1.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;pkgbuild&amp;quot;, &amp;quot;1.0.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;processx&amp;quot;, &amp;quot;3.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;purrr&amp;quot;, &amp;quot;0.3.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;R6&amp;quot;, &amp;quot;2.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;Rcpp&amp;quot;, &amp;quot;1.0.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;remotes&amp;quot;, &amp;quot;2.0.4&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rlang&amp;quot;, &amp;quot;0.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;roxygen2&amp;quot;, &amp;quot;6.1.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;sys&amp;quot;, &amp;quot;3.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;usethis&amp;quot;, &amp;quot;1.5.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;whisker&amp;quot;, &amp;quot;0.3-2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;xml2&amp;quot;, &amp;quot;1.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; 
#&amp;gt; # copy source packages (*.tar.gz) to container
#&amp;gt; COPY source_packages /source_packages
#&amp;gt; 
#&amp;gt; # install &amp;#39;dockr&amp;#39; package
#&amp;gt; RUN R -e &amp;#39;install.packages(pkgs = &amp;quot;source_packages/dockr_0.8.6.tar.gz&amp;quot;, repos = NULL)&amp;#39;
#&amp;gt; &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, the versions of the R packages, that will be installed in the Docker
container image, are all given explicitly. They will mirror the versions of the
dependencies, that are in fact loaded or installed on your system. In this way,
the Docker container image seeks to reflect your current R session as close as
possible and by doing so create an environment, where you will be able to
reproduce results from your current R session.&lt;/p&gt;
&lt;p&gt;Also note, that CRAN R packages will be installed from relevant
&lt;a href=&#34;https://mran.microsoft.com/&#34;&gt;MRAN snapshots&lt;/a&gt; - using the
&lt;code&gt;remotes::install_version()&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;folder-with-source-packages&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Folder with Source Packages&lt;/h3&gt;
&lt;p&gt;The ‘source_packages’ folder contains the local (non-CRAN) packages, that have
to be installed in the Docker container image in order for &lt;code&gt;dockr&lt;/code&gt; to run.&lt;/p&gt;
&lt;p&gt;Since &lt;code&gt;dockr&lt;/code&gt; does not depend on any local (non-CRAN) packages,
&lt;code&gt;source_packages&lt;/code&gt; only contains a source package version of &lt;code&gt;dockr&lt;/code&gt; itself,
i.e.:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list.files(image_dockr$paths$dir_source_packages)
#&amp;gt; [1] &amp;quot;dockr_0.8.6.tar.gz&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-edit-dockerfile-further&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to edit Dockerfile further&lt;/h2&gt;
&lt;p&gt;If there is need for adding additional lines to/editing the Dockerfile (e.g.
if you have to install any non-R &lt;a href=&#34;#nonr&#34;&gt;dependencies&lt;/a&gt;, this can
be achieved with the &lt;code&gt;write_lines_to_file()&lt;/code&gt; function. &lt;code&gt;write_lines_to_file()&lt;/code&gt;
enables you to add new lines to the beginning or the end of the Dockerfile.&lt;/p&gt;
&lt;p&gt;Let us try it out and write a couple of additional lines to the Dockerfile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# write three lines to beginning of file.
write_lines_to_file(c(&amp;quot;# set maintainer&amp;quot;,
                    &amp;quot;MAINTAINER Lars KJELDGAARD &amp;lt;lars_kjeldgaard@hotmail.com&amp;gt;&amp;quot;, 
                    &amp;quot;&amp;quot;),
                    image_dockr$paths$path_Dockerfile,
                    prepend = TRUE,
                    print_file = FALSE)

# write lines to the end of the file.
write_lines_to_file(c(&amp;quot;# check out smaakage85.netlify.com &amp;gt;:-]~~&amp;quot;),
                    image_dockr$paths$path_Dockerfile,
                    prepend = FALSE,
                    print_file = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Take a look at the resulting Dockerfile.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print_file(image_dockr$paths$path_Dockerfile)
#&amp;gt; # set maintainer
#&amp;gt; MAINTAINER Lars KJELDGAARD &amp;lt;lars_kjeldgaard@hotmail.com&amp;gt;
#&amp;gt; 
#&amp;gt; # load rocker base-R image
#&amp;gt; FROM rocker/r-ver:3.6.0
#&amp;gt; 
#&amp;gt; # install specific versions of CRAN packages from MRAN snapshots
#&amp;gt; RUN R -e &amp;#39;install.packages(&amp;quot;remotes&amp;quot;)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;askpass&amp;quot;, &amp;quot;1.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;assertthat&amp;quot;, &amp;quot;0.2.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;brew&amp;quot;, &amp;quot;1.0-6&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;clisymbols&amp;quot;, &amp;quot;1.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;commonmark&amp;quot;, &amp;quot;1.7&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;crayon&amp;quot;, &amp;quot;1.3.4&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;desc&amp;quot;, &amp;quot;1.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;fs&amp;quot;, &amp;quot;1.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;gh&amp;quot;, &amp;quot;1.0.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;glue&amp;quot;, &amp;quot;1.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;gtools&amp;quot;, &amp;quot;3.8.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;ini&amp;quot;, &amp;quot;0.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;jsonlite&amp;quot;, &amp;quot;1.6&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;magrittr&amp;quot;, &amp;quot;1.5&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;memoise&amp;quot;, &amp;quot;1.1.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;pkgload&amp;quot;, &amp;quot;1.0.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;prettyunits&amp;quot;, &amp;quot;1.0.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;ps&amp;quot;, &amp;quot;1.3.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rcmdcheck&amp;quot;, &amp;quot;1.3.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rprojroot&amp;quot;, &amp;quot;1.3-2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rstudioapi&amp;quot;, &amp;quot;0.10&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;sessioninfo&amp;quot;, &amp;quot;1.1.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;stringi&amp;quot;, &amp;quot;1.4.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;stringr&amp;quot;, &amp;quot;1.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;withr&amp;quot;, &amp;quot;2.1.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;xopen&amp;quot;, &amp;quot;1.0.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;yaml&amp;quot;, &amp;quot;2.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;backports&amp;quot;, &amp;quot;1.1.4&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;callr&amp;quot;, &amp;quot;3.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;cli&amp;quot;, &amp;quot;1.1.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;clipr&amp;quot;, &amp;quot;0.6.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;curl&amp;quot;, &amp;quot;3.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;devtools&amp;quot;, &amp;quot;2.0.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;digest&amp;quot;, &amp;quot;0.6.20&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;git2r&amp;quot;, &amp;quot;0.25.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;httr&amp;quot;, &amp;quot;1.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;mime&amp;quot;, &amp;quot;0.6&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;openssl&amp;quot;, &amp;quot;1.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;pkgbuild&amp;quot;, &amp;quot;1.0.3&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;processx&amp;quot;, &amp;quot;3.3.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;purrr&amp;quot;, &amp;quot;0.3.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;R6&amp;quot;, &amp;quot;2.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;Rcpp&amp;quot;, &amp;quot;1.0.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;remotes&amp;quot;, &amp;quot;2.0.4&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;rlang&amp;quot;, &amp;quot;0.4.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;roxygen2&amp;quot;, &amp;quot;6.1.1&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;sys&amp;quot;, &amp;quot;3.2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;usethis&amp;quot;, &amp;quot;1.5.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;whisker&amp;quot;, &amp;quot;0.3-2&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; RUN R -e &amp;#39;remotes::install_version(&amp;quot;xml2&amp;quot;, &amp;quot;1.2.0&amp;quot;, dependencies = FALSE)&amp;#39;
#&amp;gt; 
#&amp;gt; # copy source packages (*.tar.gz) to container
#&amp;gt; COPY source_packages /source_packages
#&amp;gt; 
#&amp;gt; # install &amp;#39;dockr&amp;#39; package
#&amp;gt; RUN R -e &amp;#39;install.packages(pkgs = &amp;quot;source_packages/dockr_0.8.6.tar.gz&amp;quot;, repos = NULL)&amp;#39;
#&amp;gt; 
#&amp;gt; # check out smaakage85.netlify.com &amp;gt;:-]~~&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dealing-with-local-non-cran-r-package-dependencies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dealing with local non-CRAN R package dependencies&lt;/h2&gt;
&lt;p&gt;If your package depends on local non-CRAN R packages, &lt;code&gt;dockr&lt;/code&gt; will also include
these packages in the Docker container image. Local non-CRAN R packages must be
available as source packages ([packageName]_[packageVersion].tar.gz) in one
or more user specified local directories. These paths have to be specified in the
‘dir_src’ argument, when invoking the &lt;code&gt;prepare_docker_image()&lt;/code&gt;, e.g.:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# image for my package &amp;#39;recorder&amp;#39;.
image_recorder &amp;lt;- prepare_docker_image(&amp;quot;~/recorder&amp;quot;,
                                       dir_image = &amp;quot;~&amp;quot;,
                                       dir_install = &amp;quot;auto&amp;quot;,
                                       dir_src = c(&amp;quot;~/src&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note, that you can store &lt;strong&gt;multiple&lt;/strong&gt; versions of the same package in your local
repos. In this way ‘dockr’ comes with a lot of flexibility.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-non-r-dependencies&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What about non-R dependencies?&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;dockr&lt;/code&gt; does &lt;em&gt;not&lt;/em&gt; deal with any non-R dependencies what so ever at this point.
In case that, for instance, your package has any Linux specific dependencies,
you will have to install them yourself in the Docker container image.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;contact&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Contact&lt;/h2&gt;
&lt;p&gt;I hope, that you will find &lt;code&gt;dockr&lt;/code&gt; useful.&lt;/p&gt;
&lt;p&gt;Please direct any questions and feedbacks to &lt;a href=&#34;mailto:lars_kjeldgaard@hotmail.com&#34;&gt;me&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;If you want to contribute, open a &lt;a href=&#34;https://github.com/smaakage85/dockr/pulls&#34;&gt;PR&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you encounter a bug or want to suggest an enhancement, please &lt;a href=&#34;https://github.com/smaakage85/dockr/issues&#34;&gt;open an issue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Best,
smaakagen&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>&#34;trimmer&#34; - trim an R object</title>
      <link>/2019/12/19/trimmer-trim-an-r-object/</link>
      <pubDate>Thu, 19 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/19/trimmer-trim-an-r-object/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/smaakage85/trimmer/master/man/figures/hex_trimmer.png&#34; align=&#34;right&#34; height=250/&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;trimmer&lt;/code&gt; 0.7.5 is now available on CRAN.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;trimmer&lt;/code&gt; is a lightweight toolkit to trim a (potentially big) R object without
breaking the results of a given function call, where the (trimmed) R object
is given as argument.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;trim&lt;/code&gt; function is the bread and butter of &lt;code&gt;trimmer&lt;/code&gt;. It seeks to reduce
the size of an R object by recursively removing elements from the object
one-by-one. It does so in a ‘greedy’ fashion - it constantly tries to
remove the element that uses the most memory.&lt;/p&gt;
&lt;p&gt;The trimming process is constrained by a reference function call. The trimming
procedure will not allow elements to be removed from the object, that will
cause results from the function call to diverge from the original results of
the function call.&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/4ABWPP59ItFKykdaDF09K5&#34; width=&#34;300&#34; height=&#34;80&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;There can be many data reasons as to why, you might want to ‘trim’ an R object.&lt;/p&gt;
&lt;p&gt;A typical example could be a R model object. It will typically contain all kinds
of (more or less useful) stuff and meta data with information about the model.
You might want to try to reduce the size of the object for (memory) efficiency purposes,
such that the model only contains only what is in fact needed to predict
new observations - and &lt;em&gt;nothing&lt;/em&gt; else!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;installation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Installation&lt;/h2&gt;
&lt;p&gt;Install the development version of &lt;code&gt;trimmer&lt;/code&gt; with:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;remotes::install_github(&amp;quot;smaakage85/trimmer&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or install the version released on CRAN:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;trimmer&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;trimming-process&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Trimming Process&lt;/h2&gt;
&lt;p&gt;The trimming procedure - conducted with &lt;code&gt;trim()&lt;/code&gt; - consists of the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Call the specified function with the object before trimming and save results for reference.&lt;/li&gt;
&lt;li&gt;Compute size of elements in the most shallow layer of object. These elements are the candidates for elimination.&lt;/li&gt;
&lt;li&gt;Identify the candidate element that uses most memory.&lt;/li&gt;
&lt;li&gt;Call function again, but this time with the element from step 3 removed from object.&lt;/li&gt;
&lt;li&gt;If results from function call are the same as in step 1, remove element from object. If results diverge, keep element - and expand the list with candidates for elimination with elements of most shallow layer of this object (if there are any).&lt;/li&gt;
&lt;li&gt;Repeat steps 3 to 5, until target size is reached or until no further
elements can be removed without results from function call diverge.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;workflow-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Workflow Example&lt;/h2&gt;
&lt;p&gt;Get ready by loading the package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(trimmer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Train a model on the famous &lt;code&gt;mtcars&lt;/code&gt; data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load training data.
trn &amp;lt;- datasets::mtcars

# estimate model.
mdl &amp;lt;- lm(mpg ~ ., data = trn)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I want to trim the model object &lt;code&gt;mdl&lt;/code&gt; as possible without affecting the predictions,
computed with function &lt;code&gt;predict()&lt;/code&gt;, for the resulting model.&lt;/p&gt;
&lt;p&gt;The trimming is then simply conducted by invoking:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mdl_trim &amp;lt;- trim(obj = mdl,
                 obj_arg_name = &amp;quot;object&amp;quot;,
                 fun = predict,
                 newdata = trn)
#&amp;gt; * Initial object size: 22.22 kB
#&amp;gt; Begin trimming object.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;model&amp;#39;)]], element size = 14.05 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 18.19 kB [v4.03 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;)]], element size = 7.79 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;terms&amp;#39;)]], element size = 7.63 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;qr&amp;#39;)]], element size = 6.66 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 14.95 kB [v7.27 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;residuals&amp;#39;)]], element size = 2.86 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 14.53 kB [v7.7 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;fitted.values&amp;#39;)]], element size = 2.86 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 11.66 kB [v10.56 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;effects&amp;#39;)]], element size = 1.4 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 10.76 kB [v11.46 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;coefficients&amp;#39;)]], element size = 1.09 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;call&amp;#39;)]], element size = 728 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 10.09 kB [v12.14 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;xlevels&amp;#39;)]], element size = 208 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 9.85 kB [v12.38 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;qraux&amp;#39;)]], element size = 176 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 9.62 kB [v12.61 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;assign&amp;#39;)]], element size = 96 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 9.46 kB [v12.76 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;pivot&amp;#39;)]], element size = 96 B
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;rank&amp;#39;)]], element size = 56 B
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;df.residual&amp;#39;)]], element size = 56 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 9.31 kB [v12.91 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;tol&amp;#39;)]], element size = 56 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 9.17 kB [v13.06 kB]
#&amp;gt; Trimming completed.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that’s it!&lt;/p&gt;
&lt;p&gt;Note, that I provide the &lt;code&gt;trim&lt;/code&gt; function with the extra argument &lt;code&gt;newdata&lt;/code&gt;, that
is passed to the function call with &lt;code&gt;fun&lt;/code&gt;. This means, that the trimming is
constrained by, that the results of ‘fun’ (=&lt;code&gt;predict&lt;/code&gt;) &lt;em&gt;MUST&lt;/em&gt; be exactly the same
on these data before and after the trimming.&lt;/p&gt;
&lt;p&gt;The trimmed model object now measures 9.17 kB. The original
object measured 22.22 kB.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-target-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set Target Size&lt;/h2&gt;
&lt;p&gt;If you just want the object size to be below some threshold, you can set that
as a criterion. The ‘trimming’ process will continue no further, when this
threshold is reached. This approach can be time-saving compared to
minimizing the object as much as possible (=default setting).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mdl_trim &amp;lt;- trim(obj = mdl,
                 obj_arg_name = &amp;quot;object&amp;quot;,
                 fun = predict,
                 newdata = trn,
                 size_target = 0.015)
#&amp;gt; * Initial object size: 22.22 kB
#&amp;gt; * Target object size: &amp;lt;= 15 kB
#&amp;gt; Begin trimming object.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;model&amp;#39;)]], element size = 14.05 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 18.19 kB [v4.03 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;)]], element size = 7.79 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;terms&amp;#39;)]], element size = 7.63 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;qr&amp;#39;)]], element size = 6.66 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 14.95 kB [v7.27 kB]
#&amp;gt; Trimming completed.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these settings, the trimmed model object measures 14.95 kB. The original
object measured 22.22 kB.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-applications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Applications&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;trimmer&lt;/code&gt; is compatible with all R objects, that inherit from the &lt;code&gt;list&lt;/code&gt;
class - not just R model objects - and all kinds of functions - not just the
&lt;code&gt;predict&lt;/code&gt; function. Hence &lt;code&gt;trimmer&lt;/code&gt; is quite a flexible tool.&lt;/p&gt;
&lt;p&gt;To illustrate I will trim the same object but under the constraint, that the
results from the &lt;code&gt;summary()&lt;/code&gt; function must be preserved.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mdl_trim &amp;lt;- trim(obj = mdl,
                 obj_arg_name = &amp;quot;object&amp;quot;,
                 fun = summary)
#&amp;gt; * Initial object size: 22.22 kB
#&amp;gt; Begin trimming object.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;model&amp;#39;)]], element size = 14.05 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 18.19 kB [v4.03 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;)]], element size = 7.79 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;terms&amp;#39;)]], element size = 7.63 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;qr&amp;#39;)]], element size = 6.66 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;residuals&amp;#39;)]], element size = 2.86 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;fitted.values&amp;#39;)]], element size = 2.86 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;effects&amp;#39;)]], element size = 1.4 kB
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 17.42 kB [v4.81 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;coefficients&amp;#39;)]], element size = 1.09 kB
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;call&amp;#39;)]], element size = 728 B
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;xlevels&amp;#39;)]], element size = 208 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 17.21 kB [v5.02 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;qraux&amp;#39;)]], element size = 176 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 16.94 kB [v5.28 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;assign&amp;#39;)]], element size = 96 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 16.76 kB [v5.46 kB]
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;pivot&amp;#39;)]], element size = 96 B
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;rank&amp;#39;)]], element size = 56 B
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;df.residual&amp;#39;)]], element size = 56 B
#&amp;gt; x Element could not be removed.
#&amp;gt; ~ Trying to remove element [[c(&amp;#39;qr&amp;#39;,&amp;#39;tol&amp;#39;)]], element size = 56 B
#&amp;gt; v Element removed.
#&amp;gt; * Object size after removal: 16.65 kB [v5.58 kB]
#&amp;gt; Trimming completed.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;other-notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other Notes&lt;/h2&gt;
&lt;p&gt;You can choose, that certain elements &lt;em&gt;MUST NOT&lt;/em&gt; be removed during the trimming process. Do this with the &lt;code&gt;dont_touch&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;You can choose whether or not to tolerate warnings from reference function calls with the argument &lt;code&gt;tolerate_warnings&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-development&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Future Development&lt;/h2&gt;
&lt;p&gt;I would like to extend the framework to also support parallellization.&lt;/p&gt;
&lt;p&gt;That is it, I hope, that you will enjoy the &lt;code&gt;trimmer&lt;/code&gt; package :)&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>recorder: Validate Predictors in New Data</title>
      <link>/2019/05/21/recorder-validate-new-data-for-predictions/</link>
      <pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/05/21/recorder-validate-new-data-for-predictions/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://github.com/smaakage85/recorder/blob/master/man/figures/logo.png?raw=true&#34; align=&#34;right&#34; height=250/&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;recorder&lt;/code&gt; 0.8.1 is now available on CRAN. &lt;code&gt;recorder&lt;/code&gt; is a lightweight toolkit to
validate new observations before computing their corresponding predictions with
a predictive model.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;recorder&lt;/code&gt; the validation process consists of two steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;record relevant statistics and meta data of the variables in the original
training data for the predictive model&lt;/li&gt;
&lt;li&gt;use these data to run a set of basic validation tests on the new set of
observations.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we will take a deeper look into, what &lt;code&gt;recorder&lt;/code&gt; has to offer.&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;[PLAY]&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/0k1xMUwn9sb7bZiqdT9ygx&#34; width=&#34;300&#34; height=&#34;80&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;There can be many data specific reasons, why you might not be confident in the
predictions of a predictive model on new data.&lt;/p&gt;
&lt;p&gt;Some of them are obvious, e.g.:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One or more variables in training data are not found in new data&lt;/li&gt;
&lt;li&gt;The class of a given variable differs in training data and new data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Others are more subtle, for instance when observations in
new data are not within the “span” of the training data. One example of this could
be, when a variable is “N/A” (missing) for a new observation to be predicted,
but no missing values appeared for the same variable in the training data.
This implies, that the new observation is not within the “span” of the training
data. Another way of putting this: the model has never encountered an
observation like this before, therefore there is good reason to doubt the
quality of the prediction.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;recorder-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;recorder workflow&lt;/h2&gt;
&lt;p&gt;We will need some data in order to demonstrate the &lt;code&gt;recorder&lt;/code&gt; workflow. As so
many times before the famous &lt;code&gt;iris&lt;/code&gt; data set will be used as an example. The
data set is divided into training data, that can be used for model development,
and new data for predictions after modelling, which we can validate with
&lt;code&gt;recordr&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1)
trn_idx &amp;lt;- sample(seq_len(nrow(iris)), 100)
data_training &amp;lt;- iris[trn_idx, ]
data_new &amp;lt;- iris[-trn_idx, ]&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;record-statistics-and-meta-data-of-variables-in-training-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Record statistics and meta data of variables in training data&lt;/h3&gt;
&lt;p&gt;What we want to achieve is to validate the new observations (before computing
their predictions with a predictive model) based on relevant
statistics and meta data of the variables in the training data. Therefore
relevant statistics and meta data of the variables must first be learned
(recorded) from the trainingdata of the model. This is done with the &lt;code&gt;record()&lt;/code&gt;
function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(recorder)
tape &amp;lt;- record(data_training)
#&amp;gt; 
#&amp;gt; [RECORD]
#&amp;gt; 
#&amp;gt; ... recording meta data and statistics of 100 rows with 5 columns... 
#&amp;gt; 
#&amp;gt; [STOP]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This provides us with an object belonging to the &lt;code&gt;data.tape&lt;/code&gt; class.
The &lt;code&gt;data.tape&lt;/code&gt; contains the statistics and meta data recorded from the training
data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(tape)
#&amp;gt; List of 2
#&amp;gt;  $ class_variables:List of 5
#&amp;gt;   ..$ Sepal.Length: chr &amp;quot;numeric&amp;quot;
#&amp;gt;   ..$ Sepal.Width : chr &amp;quot;numeric&amp;quot;
#&amp;gt;   ..$ Petal.Length: chr &amp;quot;numeric&amp;quot;
#&amp;gt;   ..$ Petal.Width : chr &amp;quot;numeric&amp;quot;
#&amp;gt;   ..$ Species     : chr &amp;quot;factor&amp;quot;
#&amp;gt;  $ parameters     :List of 5
#&amp;gt;   ..$ Sepal.Length:List of 3
#&amp;gt;   .. ..$ min   : num 4.3
#&amp;gt;   .. ..$ max   : num 7.9
#&amp;gt;   .. ..$ any_NA: logi FALSE
#&amp;gt;   ..$ Sepal.Width :List of 3
#&amp;gt;   .. ..$ min   : num 2
#&amp;gt;   .. ..$ max   : num 4.2
#&amp;gt;   .. ..$ any_NA: logi FALSE
#&amp;gt;   ..$ Petal.Length:List of 3
#&amp;gt;   .. ..$ min   : num 1
#&amp;gt;   .. ..$ max   : num 6.9
#&amp;gt;   .. ..$ any_NA: logi FALSE
#&amp;gt;   ..$ Petal.Width :List of 3
#&amp;gt;   .. ..$ min   : num 0.1
#&amp;gt;   .. ..$ max   : num 2.5
#&amp;gt;   .. ..$ any_NA: logi FALSE
#&amp;gt;   ..$ Species     :List of 2
#&amp;gt;   .. ..$ levels: chr [1:3] &amp;quot;setosa&amp;quot; &amp;quot;versicolor&amp;quot; &amp;quot;virginica&amp;quot;
#&amp;gt;   .. ..$ any_NA: logi FALSE
#&amp;gt;  - attr(*, &amp;quot;class&amp;quot;)= chr [1:2] &amp;quot;list&amp;quot; &amp;quot;data.tape&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, which meta data and statistics are recorded for the individual
variables depends on the class of the given variable, e.g. for a numeric
variable &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt; values are computed, whilst &lt;code&gt;levels&lt;/code&gt; is recorded for
factor variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;validate-new-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Validate new data&lt;/h3&gt;
&lt;p&gt;First, to spice things up, we will give the new observations a twist by inserting
some extreme values and some missing values. On top of that we will create a new
column, that was not observed in training data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create sample of row indices.
samples &amp;lt;- lapply(1:3, function(x) {
  set.seed(x) 
  sample(nrow(data_new), 5, replace = FALSE)})

# create numeric values without range, -Inf and Inf.
data_new$Sepal.Width[samples[[1]]] &amp;lt;- -Inf
data_new$Petal.Width[samples[[2]]] &amp;lt;- Inf

# insert NA&amp;#39;s in numeric vector.
data_new$Petal.Length[samples[[3]]] &amp;lt;- NA_real_

# insert new column.
data_new$junk &amp;lt;- &amp;quot;junk&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we will validate the new observations by running a number of basic
validation tests on each of the new observations. The tests are based on the
&lt;code&gt;data.tape&lt;/code&gt; with the recorded statistics and meta data of variabels in the
training data.&lt;/p&gt;
&lt;p&gt;You can get an overview over the validation tests with &lt;code&gt;get_tests_meta_data()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_tests_meta_data()
#&amp;gt;           test_name evaluate_level   evaluate_class
#&amp;gt; 1: missing_variable            col              all
#&amp;gt; 2:   mismatch_class            col              all
#&amp;gt; 3:  mismatch_levels            col           factor
#&amp;gt; 4:     new_variable            col              all
#&amp;gt; 5:    outside_range            row numeric, integer
#&amp;gt; 6:        new_level            row           factor
#&amp;gt; 7:           new_NA            row              all
#&amp;gt; 8:         new_text            row        character
#&amp;gt;                                                    description
#&amp;gt; 1:  variable observed in training data but missing in new data
#&amp;gt; 2: &amp;#39;class&amp;#39; in new data does not match &amp;#39;class&amp;#39; in training data
#&amp;gt; 3:    &amp;#39;levels&amp;#39; in new data and training data are not identical
#&amp;gt; 4:      variable observed in new data but not in training data
#&amp;gt; 5:   value in new data outside recorded range in training data
#&amp;gt; 6:           new &amp;#39;level&amp;#39; in new data compared to training data
#&amp;gt; 7:            NA observed in new data but not in training data
#&amp;gt; 8:              new text in new data compared to training data&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To run the tests simply invoke the &lt;code&gt;play()&lt;/code&gt; function with the recorded &lt;code&gt;data.tape&lt;/code&gt;
on the new data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;playback &amp;lt;- play(tape, data_new)
#&amp;gt; 
#&amp;gt; [PLAY]
#&amp;gt; 
#&amp;gt; ... playing data.tape on new data with 50 rows with 6 columns ...
#&amp;gt; 
#&amp;gt; [STOP]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we actually have here is an object belonging to the new &lt;code&gt;data.playback&lt;/code&gt;
class.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(playback)
#&amp;gt; [1] &amp;quot;data.playback&amp;quot; &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great, now let us have a detailed look at the test results with the &lt;code&gt;print()&lt;/code&gt;
method.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;playback
#&amp;gt; 
#&amp;gt; [PLAY]
#&amp;gt; 
#&amp;gt; # of rows in new data: 50
#&amp;gt; # of rows passing all tests: 0
#&amp;gt; # of rows failing one or more tests: 50
#&amp;gt; 
#&amp;gt; Test results (failures):
#&amp;gt; &amp;gt; &amp;#39;missing_variable&amp;#39;: no failures
#&amp;gt; &amp;gt; &amp;#39;mismatch_class&amp;#39;: no failures
#&amp;gt; &amp;gt; &amp;#39;mismatch_levels&amp;#39;: no failures
#&amp;gt; &amp;gt; &amp;#39;new_variable&amp;#39;: junk
#&amp;gt; &amp;gt; &amp;#39;outside_range&amp;#39;: Sepal.Width[row(s): #1, #4, #7, #23, #34, #39],
#&amp;gt; Petal.Width[row(s): #6, #15, #21, #32, #48]
#&amp;gt; &amp;gt; &amp;#39;new_level&amp;#39;: no failures
#&amp;gt; &amp;gt; &amp;#39;new_NA&amp;#39;: Petal.Length[row(s): #5, #12, #36, #39, #40]
#&amp;gt; &amp;gt; &amp;#39;new_text&amp;#39;: no failures
#&amp;gt; 
#&amp;gt; Test descriptions:
#&amp;gt; &amp;#39;missing_variable&amp;#39;: variable observed in training data but missing in new data
#&amp;gt; &amp;#39;mismatch_class&amp;#39;: &amp;#39;class&amp;#39; in new data does not match &amp;#39;class&amp;#39; in training data
#&amp;gt; &amp;#39;mismatch_levels&amp;#39;: &amp;#39;levels&amp;#39; in new data and training data are not identical
#&amp;gt; &amp;#39;new_variable&amp;#39;: variable observed in new data but not in training data
#&amp;gt; &amp;#39;outside_range&amp;#39;: value in new data outside recorded range in training data
#&amp;gt; &amp;#39;new_level&amp;#39;: new &amp;#39;level&amp;#39; in new data compared to training data
#&amp;gt; &amp;#39;new_NA&amp;#39;: NA observed in new data but not in training data
#&amp;gt; &amp;#39;new_text&amp;#39;: new text in new data compared to training data
#&amp;gt; 
#&amp;gt; [STOP]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, we are in a lot of trouble here. All rows failed, because
a new variable (&lt;code&gt;junk&lt;/code&gt;), that did not appear in the training data, was
suddenly observed in new data. By assumption this invalidates all rows.&lt;/p&gt;
&lt;p&gt;Besides from that, some rows failed, because values &lt;code&gt;Inf&lt;/code&gt; and &lt;code&gt;-Inf&lt;/code&gt; were outside
the recorded range in the training data for variables &lt;code&gt;Sepal.Width&lt;/code&gt; and
&lt;code&gt;Petal.Width&lt;/code&gt;. Also, a handful of &lt;code&gt;NA&lt;/code&gt; values were encountered in new data
for &lt;code&gt;Petal.Length&lt;/code&gt;. This is a new phenomenon compared to the training data,
where no &lt;code&gt;NA&lt;/code&gt; values were observed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extract-test-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extract test results&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;recorder&lt;/code&gt; allows you extract the results of the validation tests in a number
of ways.&lt;/p&gt;
&lt;div id=&#34;get-failed-tests-as-data.frame&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Get failed tests as data.frame&lt;/h4&gt;
&lt;p&gt;You might want to extract the results as a data.frame with the results of the
(failed) tests as columns. To do this, invoke &lt;code&gt;get_failed_tests()&lt;/code&gt; on
&lt;code&gt;playback&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(get_failed_tests(playback), 15))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;outside_range.Sepal.Width&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;outside_range.Petal.Width&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;new_NA.Petal.Length&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;new_variable.junk&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;get-failed-tests-as-character&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Get failed tests as character&lt;/h4&gt;
&lt;p&gt;It can also be useful to get the results of the (failed) tests as a string with
one entry per row in new data, where names of the failed tests for the given
row are concatenated.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(get_failed_tests_string(playback))
#&amp;gt; [1] &amp;quot;outside_range.Sepal.Width;new_variable.junk;&amp;quot;
#&amp;gt; [2] &amp;quot;new_variable.junk;&amp;quot;                          
#&amp;gt; [3] &amp;quot;new_variable.junk;&amp;quot;                          
#&amp;gt; [4] &amp;quot;outside_range.Sepal.Width;new_variable.junk;&amp;quot;
#&amp;gt; [5] &amp;quot;new_NA.Petal.Length;new_variable.junk;&amp;quot;      
#&amp;gt; [6] &amp;quot;outside_range.Petal.Width;new_variable.junk;&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;get-clean-rows&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Get clean rows&lt;/h4&gt;
&lt;p&gt;As a third option you can extract a logical vector, that indicates which rows,
that passed the validation tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_clean_rows(playback)
#&amp;gt;  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
#&amp;gt; [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
#&amp;gt; [23] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
#&amp;gt; [34] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
#&amp;gt; [45] FALSE FALSE FALSE FALSE FALSE FALSE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;TRUE&lt;/code&gt; means, that a given row is clean and has passed all tests, &lt;code&gt;FALSE&lt;/code&gt;
on the other hand implies that a given row failed one or more tests.&lt;/p&gt;
&lt;p&gt;In this case, all rows are invalid due to the strange column
&lt;code&gt;junk&lt;/code&gt;, that appears in the new data (you might think, this is a strict rule,
but it is consistent nonetheless).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;ignore-specific-test-results&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ignore specific test results&lt;/h3&gt;
&lt;p&gt;It might be, that the user - for various reasons - wants to ignore one or more
of the failed tests. You can handle this easily with &lt;code&gt;recorder&lt;/code&gt;, whenever you
invoke one of the functions &lt;code&gt;get_clean_rows()&lt;/code&gt;, &lt;code&gt;get_failed_tests()&lt;/code&gt; or
&lt;code&gt;get_failed_tests_string()&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;ignore-test-results-from-specific-tests&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ignore test results from specific tests&lt;/h4&gt;
&lt;p&gt;Let us assume, that we do not care about, if there is a new column in
the new data, that was not observed in the training data. The results of a
specific test can be ignored with the &lt;code&gt;ignore_tests&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;Let us try it out and ignore the results of the &lt;code&gt;new_variable&lt;/code&gt; validation test.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_clean_rows(playback, ignore_tests = &amp;quot;new_variable&amp;quot;)
#&amp;gt;  [1] FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE
#&amp;gt; [12] FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE
#&amp;gt; [23] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE
#&amp;gt; [34] FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE
#&amp;gt; [45]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;According to this - less restrictive - selection, 35
of the new observations are now valid.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ignore-test-results-from-tests-of-specific-columns&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ignore test results from tests of specific columns&lt;/h4&gt;
&lt;p&gt;Maybe you - for some reason - do not care about the tests results for a specific
column. You can ignore results from tests of a specific variable with the
&lt;code&gt;ignore_cols&lt;/code&gt; argument. Let us go ahead and suppress the test results from
tests of the &lt;code&gt;Petal.Length&lt;/code&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;get_clean_rows(playback, 
               ignore_tests = &amp;quot;new_variable&amp;quot;,
               ignore_cols = &amp;quot;Petal.Length&amp;quot;)
#&amp;gt;  [1] FALSE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE
#&amp;gt; [12]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE
#&amp;gt; [23] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE
#&amp;gt; [34] FALSE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
#&amp;gt; [45]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, with this modification a total of 39
of the new observations are now valid.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ignore-test-results-from-specific-tests-of-specific-columns&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Ignore test results from specific tests of specific columns&lt;/h4&gt;
&lt;p&gt;It is also possible to ignore the test results of specific tests of specific
columns with the &lt;code&gt;ignore_combinations&lt;/code&gt; argument. Let us try to ignore the
&lt;code&gt;outside_range&lt;/code&gt; test, but only for the &lt;code&gt;Sepal.Width&lt;/code&gt; variable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable(head(get_failed_tests(playback, 
                                   ignore_tests = &amp;quot;new_variable&amp;quot;,
                                   ignore_cols = &amp;quot;Petal.Length&amp;quot;,
                                   ignore_combinations = list(outside_range = &amp;quot;Sepal.Width&amp;quot;)),
                  15))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;outside_range.Petal.Width&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TRUE&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As you see - with this additional removal - the only test failures that remain
are the ones from the &lt;code&gt;outside_range&lt;/code&gt; test of the &lt;code&gt;Petal.Width&lt;/code&gt; variable.&lt;/p&gt;
&lt;p&gt;That is it, I hope, that you will enjoy the &lt;code&gt;recorder&lt;/code&gt; package :)&lt;/p&gt;
&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;&lt;strong&gt;[STOP]&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>One Recipe Step to Rule Them All</title>
      <link>/2018/12/03/one-recipe-step-to-rule-them-all/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/03/one-recipe-step-to-rule-them-all/</guid>
      <description>&lt;p&gt;In this post I will demonstrate, how my new R package &lt;code&gt;customsteps&lt;/code&gt; can be used to create recipe steps, that apply custom transformations to a data set.&lt;/p&gt;
&lt;p&gt;Note, you should already be fairly familiar with the &lt;a href=&#34;https://cran.r-project.org/web/packages/recipes/recipes.pdf&#34;&gt;&lt;code&gt;recipes&lt;/code&gt;&lt;/a&gt; package before you continue reading this post or give &lt;code&gt;customsteps&lt;/code&gt; a spin!&lt;/p&gt;
&lt;p&gt;Recommended music for this reading session:&lt;/p&gt;
&lt;iframe src=&#34;https://open.spotify.com/embed/track/323v1HB9ugfmQyqZwFaZwz&#34; width=&#34;300&#34; height=&#34;80&#34; frameborder=&#34;0&#34; allowtransparency=&#34;true&#34; allow=&#34;encrypted-media&#34;&gt;
&lt;/iframe&gt;
&lt;div id=&#34;introducing-the-customsteps-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introducing the &lt;code&gt;customsteps&lt;/code&gt; package&lt;/h2&gt;
&lt;p&gt;Along with the &lt;code&gt;recipes&lt;/code&gt; package distribution comes a number of pre-specified steps, that enables the user to manipulate data sets in various ways. The resulting data sets (/design matrices) can then be used as inputs into statistical or machine learning models.&lt;/p&gt;
&lt;p&gt;If you want to apply a specific transformation to your data set, that is not supported by the pre-specified steps, you have two options. You can write an entire &lt;a href=&#34;https://github.com/tidymodels/recipes/blob/master/vignettes/Custom_Steps.Rmd&#34;&gt;custom recipe step &lt;strong&gt;from scratch&lt;/strong&gt;&lt;/a&gt;. This however takes quite a bit of work and code. An alternative - and sometimes better - approach is to apply the &lt;code&gt;customsteps&lt;/code&gt; package, that I have just released on CRAN.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;customsteps&amp;quot;)
library(customsteps)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;customizable-higher-order-steps&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Customizable Higher-Order Steps&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;customsteps&lt;/code&gt; contains a set of customizable higher-order recipe step functions, that create specifications of recipe steps, that will transform or filter the data in accordance with custom input functions.&lt;/p&gt;
&lt;p&gt;Let me just remind you of the definition of &lt;a href=&#34;https://en.wikipedia.org/wiki/Higher-order_function&#34;&gt;&lt;strong&gt;higher-order functions&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;In mathematics and computer science, a higher-order function is a function that does at least one of the following: 1. takes one or more functions as arguments, 2. returns a function as its result.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Next, I will present an example of how to use the &lt;code&gt;customsteps&lt;/code&gt; package in order to create a recipe step, that will apply a custom transformation to a data set.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;use-case-centering-and-scaling-numeric-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use Case: Centering and Scaling Numeric Data&lt;/h2&gt;
&lt;p&gt;Assume, that I want to transform a variable &lt;span class=&#34;math inline&#34;&gt;\({\mathbf{x}}\)&lt;/span&gt; like this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Center &lt;span class=&#34;math inline&#34;&gt;\({\mathbf{x}}\)&lt;/span&gt; around an arbitrary number &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Scale the transformed variable, such that its standard deviation equals an arbitrary number &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The transformed variable &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{x}}\)&lt;/span&gt; can then be derived as (try to do it yourself):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{x}} = \alpha + (\mathbf{x} - \bar{\mathbf{x}})\frac{\beta}{s_\mathbf{x}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mathbf{x}}\)&lt;/span&gt; is the mean of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(s_\mathbf{x}\)&lt;/span&gt; is the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\({\mathbf{x}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Note that centering &lt;span class=&#34;math inline&#34;&gt;\({\mathbf{x}}\)&lt;/span&gt; around 0 and scaling it in order to arrive at a standard deviation of 1 is just a special case of the above transformation with parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0, \beta = 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;write-the-prep-helper-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Write the &lt;code&gt;prep&lt;/code&gt; helper function&lt;/h3&gt;
&lt;p&gt;First, I need to write a function, that estimates the relevant statistical parameters from an initial data set. I call this function the &lt;code&gt;prep&lt;/code&gt; helper function.&lt;/p&gt;
&lt;p&gt;Obviously, the above transformation requires the mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{\mathbf{x}}\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(s_\mathbf{x}\)&lt;/span&gt; to be learned from the initial data set. Therefore I define a function &lt;code&gt;compute_means_sd&lt;/code&gt;, that estimates the two parameters for (any arbitrary number of) numeric variables.&lt;/p&gt;
&lt;p&gt;By convention the &lt;code&gt;prep&lt;/code&gt; helper function must take the argument &lt;code&gt;x&lt;/code&gt;: the subset of selected variables from the initial data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)

compute_means_sd &amp;lt;- function(x) {
  
  map(.x = x, ~ list(mean = mean(.x), sd = sd(.x)))

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us see the function in action. I will apply it to a subset of the famous &lt;code&gt;mtcars&lt;/code&gt; data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

# divide &amp;#39;mtcars&amp;#39; into two data sets.
cars_initial &amp;lt;- mtcars[1:16, ]
cars_new &amp;lt;- mtcars[17:nrow(mtcars), ]

# learn parameters from initial data set.
params &amp;lt;- cars_initial %&amp;gt;%
  select(mpg, disp) %&amp;gt;%
  compute_means_sd(.)

# display parameters. 
as.data.frame(params)
#&amp;gt;   mpg.mean  mpg.sd disp.mean disp.sd
#&amp;gt; 1     18.2 4.14761  250.8187 113.372&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It works like a charm. Great, we are halfway there!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;write-the-bake-helper-function&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Write the &lt;code&gt;bake&lt;/code&gt; helper function&lt;/h3&gt;
&lt;p&gt;Second, I have to specify a &lt;code&gt;bake&lt;/code&gt; helper function, that defines how to apply the transformation to a new data set using the parameters estimated from the intial data set.&lt;/p&gt;
&lt;p&gt;By convention the &lt;code&gt;bake&lt;/code&gt; helper function must take the following arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt;: the new data set, that the step will be applied to.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;prep_output&lt;/code&gt;: the output from the &lt;code&gt;prep&lt;/code&gt; helper function containing any parameters estimated from the initial data set.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I define the function &lt;code&gt;center_scale&lt;/code&gt;, that will serve as my &lt;code&gt;bake&lt;/code&gt; helper function. It will center and scale variables of a new data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;center_scale &amp;lt;- function(x, prep_output, alpha, beta) {

  # extract only the relevant variables from the new data set.
  new_data &amp;lt;- select(x, names(prep_output))

  # apply transformation to each of these variables.
  # variables are centered around &amp;#39;alpha&amp;#39; and scaled to have a standard 
  # deviation of &amp;#39;beta&amp;#39;.
  map2(.x = new_data,
       .y = prep_output,
       ~ alpha + (.x - .y$mean) * beta / .y$sd)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My first (sanity) check of the function is to apply it to the initial data set, that was used for estimation of the means and standard deviations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tibble)
# center and scale variables of new data set to have a mean of zero
# and a standard deviation of one.
cars_initial_transformed &amp;lt;- center_scale(x = cars_initial, 
                                         prep_output = params,
                                         alpha = 0, 
                                         beta = 1)

# display transformed variables.
cars_initial_transformed %&amp;gt;%
  compute_means_sd(.) %&amp;gt;%
  as.data.frame(.)
#&amp;gt;       mpg.mean mpg.sd    disp.mean disp.sd
#&amp;gt; 1 1.731877e-16      1 7.199102e-17       1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results are correct within computational precision.&lt;/p&gt;
&lt;p&gt;Also, I will just check the function out on the other subset of &lt;code&gt;mtcars&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# center and scale variables of new data set to have a mean of zero
# and a standard deviation of one.
cars_new_transformed &amp;lt;- center_scale(x = cars_new, 
                                     prep_output = params,
                                     alpha = 0, 
                                     beta = 1)

# display transformed variables.
cars_new_transformed %&amp;gt;%
  as.tibble(.) %&amp;gt;%
  head(.)
#&amp;gt; # A tibble: 6 x 2
#&amp;gt;      mpg   disp
#&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
#&amp;gt; 1 -0.844  1.67 
#&amp;gt; 2  3.42  -1.52 
#&amp;gt; 3  2.94  -1.54 
#&amp;gt; 4  3.79  -1.59 
#&amp;gt; 5  0.796 -1.15 
#&amp;gt; 6 -0.651  0.593&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks right! All that is left now is to put the pieces together into my new very own custom recipe step.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-the-pieces-together&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Putting the pieces together&lt;/h3&gt;
&lt;p&gt;The function &lt;code&gt;step_custom_transformation&lt;/code&gt; takes &lt;code&gt;prep&lt;/code&gt; and &lt;code&gt;bake&lt;/code&gt; helper functions as inputs and turns them into a complete recipe step, that can be used out of the box.&lt;/p&gt;
&lt;p&gt;I create the specification of the recipe step from the new functions &lt;code&gt;compute_means_sd&lt;/code&gt; and &lt;code&gt;center_scale&lt;/code&gt; by invoking &lt;code&gt;step_custom_transformation&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(recipes)
rec &amp;lt;- recipe(cars_initial) %&amp;gt;%
  step_custom_transformation(mpg, disp,
                             prep_function = compute_means_sd,
                             bake_function = center_scale,
                             bake_options = list(alpha = 0, beta = 1),
                             bake_how = &amp;quot;replace&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that is all there is to it! Easy.&lt;/p&gt;
&lt;p&gt;Note, by setting ‘bake_options’ to “replace”, the selected terms will be replaced with the transformed variables, when the recipe is baked.&lt;/p&gt;
&lt;p&gt;I will just check, that the recipe works as expected. First I will &lt;code&gt;prep&lt;/code&gt;(/train) the recipe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# prep recipe.
rec &amp;lt;- prep(rec)

# print recipe.
rec
#&amp;gt; Data Recipe
#&amp;gt; 
#&amp;gt; Inputs:
#&amp;gt; 
#&amp;gt;   11 variables (no declared roles)
#&amp;gt; 
#&amp;gt; Training data contained 16 data points and no missing data.
#&amp;gt; 
#&amp;gt; Operations:
#&amp;gt; 
#&amp;gt; The following variables are used for computing transformations
#&amp;gt;  and will be dropped afterwards:
#&amp;gt;  mpg, disp&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I will go right ahead and bake the new recipe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bake recipe.
cars_baked &amp;lt;- rec %&amp;gt;%
  bake(cars_new) %&amp;gt;%
  select(mpg, disp)
  
# display results.
cars_baked %&amp;gt;%
  head(.)
#&amp;gt; # A tibble: 6 x 2
#&amp;gt;      mpg   disp
#&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
#&amp;gt; 1 -0.844  1.67 
#&amp;gt; 2  3.42  -1.52 
#&amp;gt; 3  2.94  -1.54 
#&amp;gt; 4  3.79  -1.59 
#&amp;gt; 5  0.796 -1.15 
#&amp;gt; 6 -0.651  0.593&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Results are as expected (same as before). Great succes!&lt;/p&gt;
&lt;p&gt;You should now be able to create your very own recipe steps to do (almost) whatever transformation you want to your data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Customized recipe steps can be created with a minimum of effort and code using my new R package &lt;code&gt;customsteps&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;customsteps&lt;/code&gt; step functions are higher-order functions, that create specificiations of recipe steps from custom input functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Please let me hear from you, if you have any feedback on the package.&lt;/p&gt;
&lt;p&gt;Best, smaakagen&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tuning a data preprocessing pipeline with recipes and modelgrid</title>
      <link>/2018/09/24/tuning-a-data-preprocessing-pipeline-with-recipes-and-modelgrid/</link>
      <pubDate>Mon, 24 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/24/tuning-a-data-preprocessing-pipeline-with-recipes-and-modelgrid/</guid>
      <description>&lt;p&gt;In this post I will demonstrate, how the &lt;a href=&#34;https://cran.r-project.org/web/packages/modelgrid/index.html&#34;&gt;&lt;code&gt;modelgrid&lt;/code&gt;&lt;/a&gt; package can be used to facilitate experiments with the data preprocessing pipeline of a predictive model.&lt;/p&gt;
&lt;div id=&#34;data-preprocessing---an-integral-part-of-a-model-configuration&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data preprocessing - an integral part of a model configuration&lt;/h2&gt;
&lt;p&gt;Model tuning is not just a matter of tuning the hyperparameters of an algorithm. Since data preprocessing is also an integral part of the model development workflow, it is just as relevant to experiment with the data preprocessing pipeline of a model configuration. When “tuning” a model, the data preprocessing pipeline should therefore also be tuned.&lt;/p&gt;
&lt;p&gt;In the following I will present an example of tuning a data preprocessing pipeline using &lt;code&gt;modelgrid&lt;/code&gt; in combination with &lt;a href=&#34;https://github.com/tidymodels/recipes&#34;&gt;&lt;code&gt;recipes&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-case-cell-segmentation-in-high-content-screening&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use case: Cell Segmentation in High-Content Screening&lt;/h2&gt;
&lt;p&gt;I will use the Cell Segmentation data set described in the excellent book &lt;a href=&#34;http://appliedpredictivemodeling.com/&#34;&gt;&lt;strong&gt;‘Applied Predictive Modelling’&lt;/strong&gt;&lt;/a&gt; as an example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(AppliedPredictiveModeling)
data(segmentationOriginal)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data set consists of 2019 samples, where each sample represents a cell. Of these cells, 1300 were judged to be poorly segmented and 719 were well segmented; 1009 cells were reserved for the training set.&lt;/p&gt;
&lt;p&gt;In each cell 116 measurements were taken. They are all available as numeric predictors.&lt;/p&gt;
&lt;p&gt;For more information on the data set look &lt;a href=&#34;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-340&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Our goal is to develop a classification model, that separates the poorly segmented from the well segmented cells.&lt;/p&gt;
&lt;div id=&#34;data-at-a-glance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data at a glance&lt;/h3&gt;
&lt;p&gt;First, let us take a quick look at the data. We will do that by inspecting the between-predictor correlations of the predictors expressed by a correlation matrix of the training data set. The variables are grouped adjacent to each other according to their mutual correlations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract training data.
training &amp;lt;- filter(segmentationOriginal, Case == &amp;quot;Train&amp;quot;)

# Extract predictors.
predictors &amp;lt;- training %&amp;gt;% select(-(c(&amp;quot;Class&amp;quot;, &amp;quot;Case&amp;quot;, &amp;quot;Cell&amp;quot;))) 

# Identify variables with zero variance.
zero_variance_predictors &amp;lt;- map_lgl(predictors, ~ n_distinct(.x) == 1)

# Remove predictors with zero variance.
predictors &amp;lt;- predictors[, !zero_variance_predictors]

# Compute and plot a correlation matrix of remaining predictors.
library(corrplot)
predictors %&amp;gt;%
  cor(.) %&amp;gt;%
  corrplot(., order = &amp;quot;hclust&amp;quot;, tl.cex = .35)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-24-tuning-a-data-preprocessing-pipeline-with-recipes-and-modelgrid_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the graph, it seems that there are groups of predictors, that have strong positive correlations (dark blue).&lt;/p&gt;
&lt;p&gt;There can be good reasons for avoiding variables, that are highly correlated, some of them being (as stated in &lt;a href=&#34;http://appliedpredictivemodeling.com/&#34;&gt;&lt;strong&gt;‘Applied Predictive Modelling’&lt;/strong&gt;&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Redundant/highly correlated predictors often add more complexity than information to the model&lt;/li&gt;
&lt;li&gt;Mathematical disadvantages: can result in very unstable models (high variance), numerical errors and inferior predictive performance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The aim of my modelling experiments will be to apply different preprocessing techniques in order to mitigate the potential pitfalls of the “collinearity clusters”, that we are observing amongst the field of predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-initial-recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create initial recipe&lt;/h3&gt;
&lt;p&gt;First, let us set up a starting point for our data preprocessing pipeline in our modeling experiments. For this purpose I apply the &lt;strong&gt;awesome&lt;/strong&gt; &lt;code&gt;recipes&lt;/code&gt; package and create a - very basic - recipe, that will serve as an anchor for my model configurations.&lt;/p&gt;
&lt;p&gt;In this recipe I declare the roles of all variables in the data set and state, that variables with zero variances should be removed.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(recipes)
initial_recipe &amp;lt;- recipe(training) %&amp;gt;%
  add_role(Class, new_role = &amp;quot;outcome&amp;quot;) %&amp;gt;%
  add_role(Cell, new_role = &amp;quot;id variable&amp;quot;) %&amp;gt;%
  add_role(Case, new_role = &amp;quot;splitting indicator&amp;quot;) %&amp;gt;%
  add_role(-Class, -Cell, -Case, new_role = &amp;quot;predictor&amp;quot;) %&amp;gt;%
  step_zv(all_predictors())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can ‘prep’ the recipe and get an impression of, what it is actually doing. It seems, it removes two of the predictors due to them having variances of zero.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prep_rec &amp;lt;- prep(initial_recipe)
tidy(prep_rec, 1)
#&amp;gt; # A tibble: 2 x 1
#&amp;gt;   terms                       
#&amp;gt;   &amp;lt;chr&amp;gt;                       
#&amp;gt; 1 MemberAvgAvgIntenStatusCh2  
#&amp;gt; 2 MemberAvgTotalIntenStatusCh2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;set-up-a-model-grid&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Set up a model grid&lt;/h3&gt;
&lt;p&gt;In order to organize and structure my experiments with different data preprocessing pipelines I apply my &lt;a href=&#34;https://github.com/smaakage85/modelgrid&#34;&gt;&lt;code&gt;modelgrid&lt;/code&gt;&lt;/a&gt; package, that offers &lt;a href=&#34;http://smaakage85.netlify.com/2018/07/14/modelgrid-a-framework-for-creating-managing-and-training-multiple-models/&#34;&gt;a framework for constructing, training and managing multiple &lt;code&gt;caret&lt;/code&gt; models&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;modelgrid&lt;/code&gt; separates the specification of a(ny number of) &lt;code&gt;caret&lt;/code&gt; model(s) from the training/estimation of the model(s). By doing so, &lt;code&gt;modelgrid&lt;/code&gt; follows the same principles as the new promising package &lt;a href=&#34;https://github.com/topepo/parsnip&#34;&gt;&lt;code&gt;parsnip&lt;/code&gt;&lt;/a&gt;, which is under construction.&lt;/p&gt;
&lt;p&gt;Assume, that we want to estimate a family of Generalized Linear Models, all with different data preprocessing pipelines. I have decided on the following conditions for the model training:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply a cross-validation resampling scheme with 5 folds.&lt;/li&gt;
&lt;li&gt;Tune the models and measure performance using the standard and highly versatile ‘Area Under the Curve’ (AUC(/ROC)) metric.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I construct a &lt;strong&gt;model_grid&lt;/strong&gt; and set the settings, that by default will apply to all of my models, accordingly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(modelgrid)
library(caret)

models &amp;lt;- 
  # create empty model grid with constructor function.
  model_grid() %&amp;gt;%
  # set shared settings, that will apply to all models by default.
  share_settings(
    data = training,
    trControl = trainControl(method = &amp;quot;cv&amp;quot;,
                             number = 5,
                             summaryFunction = twoClassSummary,
                             classProbs = TRUE),
    metric = &amp;quot;ROC&amp;quot;,
    method = &amp;quot;glm&amp;quot;,
    family = binomial(link = &amp;quot;logit&amp;quot;)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We are now ready to add individual model specifications, each with their own data preprocessing pipeline to the model grid.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adding-the-first-model-specifications-to-the-model-grid&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Adding the first model specifications to the model grid&lt;/h3&gt;
&lt;p&gt;We will kick things off by adding the first model specification to my model grid. In this configuration I just apply our initial data preprocessing recipe and do no further. I will refer to this model as ‘baseline’.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models &amp;lt;- models %&amp;gt;%
  add_model(model_name = &amp;quot;baseline&amp;quot;, 
            x = initial_recipe)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One way of dealing with the potential drawbacks of the observed “collinearity clusters” is to apply a correlation filter. The correlation filter poses a heuristic approach to dealing with highly correlated predictors. It removes the predictors with the highest between-predictor correlations one at a time, until all between-predictor correlations are below some critical threshold.&lt;/p&gt;
&lt;p&gt;In order to do so, I extend my initial recipe with an additional step, that applies the correlation filter. Furthermore I will try out different values for the between-predictor correlation threshold value of the filter, essentially treating it as a hyperparameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models &amp;lt;- models %&amp;gt;%
  add_model(model_name = &amp;quot;corr_.7&amp;quot;, 
            x = initial_recipe %&amp;gt;%
              step_corr(all_predictors(), threshold = .7)) %&amp;gt;%
  add_model(model_name = &amp;quot;corr_.8&amp;quot;, 
            x = initial_recipe %&amp;gt;%
              step_corr(all_predictors(), threshold = .8)) %&amp;gt;%
  add_model(model_name = &amp;quot;corr_.9&amp;quot;, 
            x = initial_recipe %&amp;gt;%
              step_corr(all_predictors(), threshold = .9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The construction of these model specifications can - and indeed should - be parametrized. Especially if you want to try out a wider range of values for the ‘threshold’ parameter than just the three, that I have denoted here.&lt;/p&gt;
&lt;p&gt;Great, now we have a bunch of models specifications. We will train them right away and take a first look at the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Train models.
models &amp;lt;- models %&amp;gt;% train(.)
# Display resampled performance statistics of the fitted models using standard 
# functionality from the &amp;#39;caret&amp;#39; package.
models$model_fits %&amp;gt;% resamples(.) %&amp;gt;% bwplot(.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-24-tuning-a-data-preprocessing-pipeline-with-recipes-and-modelgrid_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Judging by the resampled AUC performance statistics it seems, that there &lt;em&gt;could&lt;/em&gt; be a case for applying a correlation filter on the set of predictors. Apparently, the model with a correlation filter with a between-predictor correlation threshold value of .7 added to the data preprocessing pipeline yields the best median resampled AUC. Of the four models, this model is by far the least complex.&lt;/p&gt;
&lt;p&gt;We can see this by taking a look at the number of predictors, that were actually used in the final models (after removing variables with a correlation filter (if any)).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models$model_fits %&amp;gt;%
  map(pluck(c(&amp;quot;recipe&amp;quot;, &amp;quot;term_info&amp;quot;, &amp;quot;role&amp;quot;))) %&amp;gt;%
  map_int(~ sum(.x == &amp;quot;predictor&amp;quot;))
#&amp;gt; baseline  corr_.7  corr_.8  corr_.9 
#&amp;gt;      114       60       78       93&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ‘corr_.7’ model configuration only uses 60 predictors (after removal of highly correlated predictors), hence it only estimates 61 coefficients. In contrast the ‘baseline’ model uses all predictors (except the two variables with zero variances) and estimates 115 coefficients in total making it a much more complex model (by means of a higher variance) and more prone to the risk of overfitting.&lt;/p&gt;
&lt;p&gt;Overall it seems like applying a correlation filter with a correlation threshold value of 0.7 as part of the data preprocessing pipeline could be a good idea.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dimensionality-reduction-with-pca&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Dimensionality reduction with PCA&lt;/h3&gt;
&lt;p&gt;Another approach to dealing with highly correlated predictors is to apply a Principal Component Analysis transformation of the predictors in order to reduce the dimensions of data set. You can read more about the PCA technique &lt;a href=&#34;https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This approach can be tested by tweaking my initial data preprocessing recipe once again with a couple of additional steps. Before actually conducting PCA, features are centered and scaled. This is completely standard.&lt;/p&gt;
&lt;p&gt;For the PCA transformation I vary the ‘threshold’ value, which is the fraction of the total variance of the predictors that should be covered by the components. The higher the value of ‘threshold’, the higher the number of components used.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extend recipe with centering and scaling steps.
rec_center_scale &amp;lt;- initial_recipe %&amp;gt;%
  step_center(all_predictors()) %&amp;gt;%
  step_scale(all_predictors())

# Add model specifications with pca for dimensionality reduction.
models &amp;lt;- models %&amp;gt;%
  add_model(model_name = &amp;quot;pca_.75&amp;quot;, 
            x = rec_center_scale %&amp;gt;%
              step_pca(all_predictors(), threshold = .75)) %&amp;gt;%
  add_model(model_name = &amp;quot;pca_.85&amp;quot;,
            x = rec_center_scale %&amp;gt;%
              step_pca(all_predictors(), threshold = .85)) %&amp;gt;%
  add_model(model_name = &amp;quot;pca_.95&amp;quot;,
            x = rec_center_scale %&amp;gt;%
              step_pca(all_predictors(), threshold = .95))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us train the new model configurations and display the results.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models &amp;lt;- train(models)
models$model_fits %&amp;gt;% caret::resamples(.) %&amp;gt;% bwplot(.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-09-24-tuning-a-data-preprocessing-pipeline-with-recipes-and-modelgrid_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Applying a data preprocessing pipeline with a PCA transformation capturing 95 pct. of the total variance of the set of predictors actually returns the highest resampled median value of AUC.&lt;/p&gt;
&lt;p&gt;You can look up, how many principal components that were used in the different model configurations in order to account for the desired amount of total variance of the predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;models$model_fits[c(&amp;quot;pca_.75&amp;quot;, &amp;quot;pca_.85&amp;quot;, &amp;quot;pca_.95&amp;quot;)] %&amp;gt;%
  map(pluck(c(&amp;quot;recipe&amp;quot;, &amp;quot;term_info&amp;quot;, &amp;quot;role&amp;quot;))) %&amp;gt;%
  map_int(~ sum(.x == &amp;quot;predictor&amp;quot;))
#&amp;gt; pca_.75 pca_.85 pca_.95 
#&amp;gt;      22      34      58&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To summarize, adding a PCA transformation or a correlation filter to the data preprocessing pipeline seem like good ways of dealing with the “collinearity clusters” in the data set.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Experimenting with the data preprocessing pipeline can be seen as part of the model tuning process. Parameters of the data preprocessing pipeline can be thought of as tuning parameters.&lt;/li&gt;
&lt;li&gt;These kinds of experiments can be organized and conducted easily using R packages &lt;code&gt;recipes&lt;/code&gt; and &lt;code&gt;caret&lt;/code&gt; in combination with &lt;code&gt;modelgrid&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;whats-next&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What’s next?&lt;/h2&gt;
&lt;p&gt;I have been thinking about how to extend the functionality of &lt;code&gt;modelgrid&lt;/code&gt; further in order to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parametrize experiments with the parameters of the data preprocessing pipeline. But actually I was under the impression, that others are working on developing similar functionality for tuning parameters of the data preprocessing pipeline. Am I wrong here?&lt;/li&gt;
&lt;li&gt;Support &lt;code&gt;parsnip&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;To expose models and model configurations from a model grid in a more ‘tidy’ way.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Best, smaakagen&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>About smaakagen</title>
      <link>/about/</link>
      <pubDate>Sat, 14 Jul 2018 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;smaakagen is an infamous data science prodigy currently working at the
Danish Tax Authorities with applications of extremely complicated algorithms on
very tiny data sets.&lt;/p&gt;

&lt;p&gt;Reach me on &lt;a href=&#34;https://twitter.com/elkjel&#34;&gt;twitter&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/lars-kjeldgaard-321b0351/&#34;&gt;LinkedIn&lt;/a&gt; or by &lt;a href=&#34;mailto:lars_kjeldgaard@hotmail.com&#34;&gt;mail&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>modelgrid - a framework for creating, managing and training multiple models</title>
      <link>/2018/07/14/modelgrid-a-framework-for-creating-managing-and-training-multiple-models/</link>
      <pubDate>Sat, 14 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/07/14/modelgrid-a-framework-for-creating-managing-and-training-multiple-models/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://cran.r-project.org/web/packages/modelgrid/index.html&#34;&gt;&lt;code&gt;modelgrid&lt;/code&gt;&lt;/a&gt; is
a new package of mine, which has just made its way on to CRAN.
&lt;code&gt;modelgrid&lt;/code&gt; offers a minimalistic but very flexible framework to create, manage
and train a portfolio of &lt;a href=&#34;https://cran.r-project.org/web/packages/modelgrid/index.html&#34;&gt;&lt;code&gt;caret&lt;/code&gt;&lt;/a&gt;
models. Note, you should already be fairly familiar with the &lt;code&gt;caret&lt;/code&gt; package
before giving &lt;code&gt;modelgrid&lt;/code&gt; a spin.&lt;/p&gt;
&lt;p&gt;Below I describe the key concept behind &lt;code&gt;modelgrid&lt;/code&gt; as well as the features of
&lt;code&gt;modelgrid&lt;/code&gt; divided into three main categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Creating a model grid&lt;/li&gt;
&lt;li&gt;Training a model grid&lt;/li&gt;
&lt;li&gt;Editing and removing models from a model grid&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After reading this post you should be able to grind models like never before
using the &lt;code&gt;modelgrid&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;key-concept-behind-the-model-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Key concept behind the model grid&lt;/h2&gt;
&lt;p&gt;When facing a Machine Learning problem, you typically want to try out a lot of
models in order to find out, what works and what does not. But how can we manage
these experiments in a structured, simple and transparent way? You guessed it -
by using the &lt;code&gt;modelgrid&lt;/code&gt; package (and yes, I am already familiar with the &lt;code&gt;caretEnsemble&lt;/code&gt;
package, but I wanted something, that was more flexible and easier/more intuitive
to work with. Also I wanted a framework, that was pipe-friendly).&lt;/p&gt;
&lt;p&gt;A tuning grid consists of combinations of hyperparameters for a specific model.
A model grid is just an extension of that concept in the sense that it
consists of - potentially many - models, each with their own tuning grid.
Basically the model grid is built by providing a set of shared settings, that by
default will apply to all models within the model grid, and defining the settings
for the individual models in the model grid.&lt;/p&gt;
&lt;p&gt;You can pre-allocate an empty model grid with the constructor function
&lt;code&gt;model_grid&lt;/code&gt; and take a look at the structure.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(modelgrid)
mg &amp;lt;- model_grid()

mg
#&amp;gt; $shared_settings
#&amp;gt; list()
#&amp;gt; 
#&amp;gt; $models
#&amp;gt; list()
#&amp;gt; 
#&amp;gt; $model_fits
#&amp;gt; list()
#&amp;gt; 
#&amp;gt; attr(,&amp;quot;class&amp;quot;)
#&amp;gt; [1] &amp;quot;model_grid&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An object belonging to the &lt;code&gt;model_grid&lt;/code&gt; class has three components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shared_settings&lt;/code&gt;: these are the settings, that will be shared by all models
in the model grid by default. Generally, it makes sense to keep some settings
fixed for all models, e.g. the choice of target variable, features, resampling scheme
and sometimes also preprocessing options. By providing them as shared settings
the user avoids redundant code.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;models&lt;/code&gt;: every individual model specification added to the model grid will be
an element in this list. The individual model specification consists of settings
that uniquely identify the indvidual model. If a setting has been set both as part
of the shared settings and the settings of a given individual model specification,
the setting from the individual model specification will apply for that given
model.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;model_fits&lt;/code&gt;: this element contains the fitted models (one for each individual
model specification), once the &lt;code&gt;model_grid&lt;/code&gt; has been trained.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-model-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating a model grid&lt;/h2&gt;
&lt;p&gt;The first natural step of setting up the model grid is to define, which settings
should be shared by all models by default. We will use the &lt;code&gt;GermanCredit&lt;/code&gt; data set
from the &lt;code&gt;caret&lt;/code&gt; package as example data and do just that with the &lt;code&gt;share_settings&lt;/code&gt;
function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(magrittr)
library(caret)
library(dplyr)
library(purrr)
# Load data on German credit applications.  
data(GermanCredit)

# Construct empty model grid and define shared settings.
mg &amp;lt;-
  model_grid() %&amp;gt;%
  share_settings(
    y = GermanCredit[[&amp;quot;Class&amp;quot;]],
    x = GermanCredit %&amp;gt;% select(-Class),
    preProc = &amp;quot;nzv&amp;quot;,
    metric = &amp;quot;ROC&amp;quot;,
    trControl = trainControl(
      method = &amp;quot;cv&amp;quot;,
      number = 5,
      summaryFunction = twoClassSummary,
      classProbs = TRUE
    )
  )

purrr::map_chr(mg$shared_settings, class)
#&amp;gt;            y            x      preProc       metric    trControl 
#&amp;gt;     &amp;quot;factor&amp;quot; &amp;quot;data.frame&amp;quot;  &amp;quot;character&amp;quot;  &amp;quot;character&amp;quot;       &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;shared_settings&lt;/code&gt; component of the model grid is now populated. In order to complete
the model grid we must define a set of individual model specifications, that
we would like to give a shot. A common choice of baseline model could be
a simple parametric model e.g. a Generalized Linear Model. The model specification
is added to the model grid with the &lt;code&gt;add_model&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mg &amp;lt;- 
  mg %&amp;gt;%
  add_model(model_name = &amp;quot;Logistic Regression Baseline&amp;quot;,
            method = &amp;quot;glm&amp;quot;,
            family = binomial(link = &amp;quot;logit&amp;quot;))

mg$models
#&amp;gt; $`Logistic Regression Baseline`
#&amp;gt; $`Logistic Regression Baseline`$method
#&amp;gt; [1] &amp;quot;glm&amp;quot;
#&amp;gt; 
#&amp;gt; $`Logistic Regression Baseline`$family
#&amp;gt; 
#&amp;gt; Family: binomial 
#&amp;gt; Link function: logit&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;model_grid&lt;/code&gt; requires a (unique) name for each individual model specification, so I
named this one ‘Logistic Regression Baseline’. If the user does not provide a name,
a generic name - ‘Model[int]’ - is generated automatically.&lt;/p&gt;
&lt;p&gt;This is all it takes to create the smallest possible model grid with only one unique
model configuration. The model grid can be trained with the &lt;code&gt;train&lt;/code&gt; function. For
more on this go to ‘Training a model grid’.&lt;/p&gt;
&lt;p&gt;But a model grid with only one model specification is obviously not a really interesting
use case. Let us insert another two model specifications into the model grid: a
set of logistic regression models, only this time with the features being preprocessed
with Principal Component Analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mg &amp;lt;- 
  mg %&amp;gt;%
  add_model(model_name = &amp;quot;Logistic Regression PCA&amp;quot;,
            method = &amp;quot;glm&amp;quot;,
            family = binomial(link = &amp;quot;logit&amp;quot;),
            preProc = c(&amp;quot;nzv&amp;quot;, &amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;, &amp;quot;pca&amp;quot;)) %&amp;gt;%
  add_model(model_name = &amp;quot;Logistic Regression PCA 98e-2&amp;quot;,
            method = &amp;quot;glm&amp;quot;,
            family = binomial(link = &amp;quot;logit&amp;quot;),
            preProc = c(&amp;quot;nzv&amp;quot;, &amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;, &amp;quot;pca&amp;quot;),
            custom_control = list(preProcOptions = list(thresh = 0.98)))
            
mg$models
#&amp;gt; $`Logistic Regression Baseline`
#&amp;gt; $`Logistic Regression Baseline`$method
#&amp;gt; [1] &amp;quot;glm&amp;quot;
#&amp;gt; 
#&amp;gt; $`Logistic Regression Baseline`$family
#&amp;gt; 
#&amp;gt; Family: binomial 
#&amp;gt; Link function: logit 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $`Logistic Regression PCA`
#&amp;gt; $`Logistic Regression PCA`$method
#&amp;gt; [1] &amp;quot;glm&amp;quot;
#&amp;gt; 
#&amp;gt; $`Logistic Regression PCA`$family
#&amp;gt; 
#&amp;gt; Family: binomial 
#&amp;gt; Link function: logit 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $`Logistic Regression PCA`$preProc
#&amp;gt; [1] &amp;quot;nzv&amp;quot;    &amp;quot;center&amp;quot; &amp;quot;scale&amp;quot;  &amp;quot;pca&amp;quot;   
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $`Logistic Regression PCA 98e-2`
#&amp;gt; $`Logistic Regression PCA 98e-2`$method
#&amp;gt; [1] &amp;quot;glm&amp;quot;
#&amp;gt; 
#&amp;gt; $`Logistic Regression PCA 98e-2`$family
#&amp;gt; 
#&amp;gt; Family: binomial 
#&amp;gt; Link function: logit 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $`Logistic Regression PCA 98e-2`$preProc
#&amp;gt; [1] &amp;quot;nzv&amp;quot;    &amp;quot;center&amp;quot; &amp;quot;scale&amp;quot;  &amp;quot;pca&amp;quot;   
#&amp;gt; 
#&amp;gt; $`Logistic Regression PCA 98e-2`$custom_control
#&amp;gt; $`Logistic Regression PCA 98e-2`$custom_control$preProcOptions
#&amp;gt; $`Logistic Regression PCA 98e-2`$custom_control$preProcOptions$thresh
#&amp;gt; [1] 0.98&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can of course add as many models as you like to the model grid with
the &lt;code&gt;add_model&lt;/code&gt; function.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training-a-model-grid&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training a model grid&lt;/h2&gt;
&lt;p&gt;The models from a model grid can be trained with the &lt;code&gt;train&lt;/code&gt; function
from the &lt;code&gt;caret&lt;/code&gt; package, for which I have implemented a S3 method for the
&lt;code&gt;model_grid&lt;/code&gt; class.&lt;/p&gt;
&lt;p&gt;When you call &lt;code&gt;train&lt;/code&gt; with a &lt;code&gt;model_grid&lt;/code&gt;, all of the individual model
specifications are consolidated with the shared settings into complete
&lt;code&gt;caret&lt;/code&gt; model specifications, which are then trained one by one with
&lt;code&gt;caret&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;consolidation-of-settings-into-complete-model-specifications&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Consolidation of settings into complete model specifications&lt;/h3&gt;
&lt;p&gt;For a given model the model settings are consolidated with the
&lt;code&gt;consolidate_model&lt;/code&gt; function. Let us see how this works with the three models.
For the baseline model there is no overlap between the shared settings and
the settings in the individual model specification, and hence the settings will
just be appended into one configuration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# there are no conflicts.
dplyr::intersect(names(mg$shared_settings), names(mg$models$`Logistic Regression Baseline`))
#&amp;gt; character(0)

# consolidate model settings into one model.
consolidate_model(
  mg$shared_settings, 
  mg$models$`Logistic Regression Baseline`
  ) %&amp;gt;%
  purrr::map_chr(class)
#&amp;gt;       method       family            y            x      preProc 
#&amp;gt;  &amp;quot;character&amp;quot;     &amp;quot;family&amp;quot;     &amp;quot;factor&amp;quot; &amp;quot;data.frame&amp;quot;  &amp;quot;character&amp;quot; 
#&amp;gt;       metric    trControl 
#&amp;gt;  &amp;quot;character&amp;quot;       &amp;quot;list&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In case the same setting has been specified both in the shared settings of the
model grid and in the individual settings for a specific model, the individual
setting will apply. This is the case for the model ‘Logistic Regression PCA’,
where the ‘preProc’ argument has also been defined in the model specific
configuration.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the &amp;#39;preProc&amp;#39; setting is defined both in the shared and model specific settings.
dplyr::intersect(names(mg$shared_settings), names(mg$models$`Logistic Regression PCA`))
#&amp;gt; [1] &amp;quot;preProc&amp;quot;

mg$shared_settings$preProc
#&amp;gt; [1] &amp;quot;nzv&amp;quot;
mg$models$`Logistic Regression PCA`$preProc
#&amp;gt; [1] &amp;quot;nzv&amp;quot;    &amp;quot;center&amp;quot; &amp;quot;scale&amp;quot;  &amp;quot;pca&amp;quot;

# consolidate model settings into one model.
consolidate_model(
  mg$shared_settings, 
  mg$models$`Logistic Regression PCA`
  ) %&amp;gt;%
  magrittr::extract2(&amp;quot;preProc&amp;quot;)
#&amp;gt; [1] &amp;quot;nzv&amp;quot;    &amp;quot;center&amp;quot; &amp;quot;scale&amp;quot;  &amp;quot;pca&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Also, if the ‘trControl’ argument is defined as part of the shared settings, the
subsettings of ‘trControl’ can be modified for a specific model with the special
setting ‘custom_control’ (which itself is given as an explicit argument to the
&lt;code&gt;add_model&lt;/code&gt; function) in the model specific settings.&lt;/p&gt;
&lt;p&gt;For the model ‘Logistic Regression PCA 98e-2’, the preprocessing options for PCA
were adjusted with ‘custom_control’. When the model is consolidated, the model
specific customizations of subsettings of the shared ‘trControl’ argument will
apply.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the &amp;#39;trControl$preProcOptions$thresh&amp;#39; setting is defined in the shared
# settings but customized in the model specific settings.
mg$shared_settings$trControl$preProcOptions$thresh
#&amp;gt; [1] 0.95
mg$models$`Logistic Regression PCA 98e-2`$custom_control$preProcOptions$thresh
#&amp;gt; [1] 0.98

# consolidate model settings into one model.
consolidate_model(
  mg$shared_settings, 
  mg$models$`Logistic Regression PCA 98e-2`
  ) %&amp;gt;%
  magrittr::extract2(c(&amp;quot;trControl&amp;quot;, &amp;quot;preProcOptions&amp;quot;, &amp;quot;thresh&amp;quot;))
#&amp;gt; [1] 0.98&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-training&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Model training&lt;/h3&gt;
&lt;p&gt;When calling the &lt;code&gt;train&lt;/code&gt; function, the &lt;code&gt;consolidate_model&lt;/code&gt; function is called
under the hood with all of the individual models and the shared settings, and
a set of complete &lt;code&gt;caret&lt;/code&gt; model specifications is generated - one for each
individual model specification.&lt;/p&gt;
&lt;p&gt;Afterwards the models are trained one by one with &lt;code&gt;caret&lt;/code&gt;, and the fitted
models are saved in the &lt;code&gt;model_fits&lt;/code&gt; component of the model grid.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train models from model grid.
mg &amp;lt;- train(mg)

# the fitted models now appear in the &amp;#39;model_fits&amp;#39; component.
names(mg$model_fits)
#&amp;gt; [1] &amp;quot;Logistic Regression Baseline&amp;quot;  &amp;quot;Logistic Regression PCA&amp;quot;      
#&amp;gt; [3] &amp;quot;Logistic Regression PCA 98e-2&amp;quot;

# extract performance.
mg$model_fits %&amp;gt;%
  caret::resamples(.) %&amp;gt;%
  lattice::bwplot(.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-14-modelgrid-a-framework-for-creating-managing-and-training-multiple-models_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we now add an additional models to the model grid, and call &lt;code&gt;train&lt;/code&gt; on the model
grid again, only the new models (those that do not yet have a fit) will be trained
by default.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# train models from model grid.
mg &amp;lt;- 
  mg %&amp;gt;%
  add_model(model_name = &amp;quot;Funky Forest&amp;quot;,
            method = &amp;quot;rf&amp;quot;) %&amp;gt;%
  train(.)

mg$model_fits %&amp;gt;%
  caret::resamples(.) %&amp;gt;%
  lattice::bwplot(.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-14-modelgrid-a-framework-for-creating-managing-and-training-multiple-models_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If you call &lt;code&gt;train&lt;/code&gt; with the ‘train_all’ argument set to &lt;code&gt;TRUE&lt;/code&gt;, all models will
be trained regardless.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;support-for-all-train-interfaces&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Support for all train interfaces&lt;/h3&gt;
&lt;p&gt;The training of a &lt;code&gt;model_grid&lt;/code&gt; supports both the explicit ‘x’, ‘y’ interface to train,
the ‘formula’ interface and last but not least the new powerful ‘recipe’ interface.
Let us try out the latter. First we will create a basic recipe.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# create basic recipe.
library(recipes)
rec &amp;lt;- 
  recipe(GermanCredit, formula = Class ~ .) %&amp;gt;%
  step_nzv(all_predictors())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With that as a starting point I will create and train a minimal model grid
as an example. I will tweak the recipe for one of the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mg_rec &amp;lt;-
  model_grid() %&amp;gt;%
  share_settings(
    metric = &amp;quot;ROC&amp;quot;,
    data = GermanCredit,
    trControl = trainControl(
      method = &amp;quot;cv&amp;quot;,
      number = 5,
      summaryFunction = twoClassSummary,
      classProbs = TRUE
    )
  ) %&amp;gt;%
  add_model(
    model_name = &amp;quot;Log Reg&amp;quot;,
    x = rec,
    method = &amp;quot;glm&amp;quot;,
    family = binomial(link = &amp;quot;logit&amp;quot;)
  ) %&amp;gt;%
  add_model(
    model_name = &amp;quot;Log Reg PCA&amp;quot;,
    x = rec %&amp;gt;%
      step_center(all_predictors()) %&amp;gt;%
      step_scale(all_predictors()) %&amp;gt;%
      step_pca(all_predictors()),
    method = &amp;quot;glm&amp;quot;,
    family = binomial(link = &amp;quot;logit&amp;quot;)
  ) %&amp;gt;%
  train(.)

mg_rec$model_fits %&amp;gt;%
  caret::resamples(.) %&amp;gt;%
  lattice::bwplot(.)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-07-14-modelgrid-a-framework-for-creating-managing-and-training-multiple-models_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;editing-and-removing-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Editing and removing models&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;modelgrid&lt;/code&gt; has a couple of functions, that makes it easy to work iteratively
with the model specifications in a model grid. If you want to modify an
existing model configuration, please use the &lt;code&gt;edit_model&lt;/code&gt; function. Below
I use it to modify one of the generalized linear models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# existing model configuration.
mg$models$`Logistic Regression PCA`
#&amp;gt; $method
#&amp;gt; [1] &amp;quot;glm&amp;quot;
#&amp;gt; 
#&amp;gt; $family
#&amp;gt; 
#&amp;gt; Family: binomial 
#&amp;gt; Link function: logit 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $preProc
#&amp;gt; [1] &amp;quot;nzv&amp;quot;    &amp;quot;center&amp;quot; &amp;quot;scale&amp;quot;  &amp;quot;pca&amp;quot;

# edit model configuration.
mg &amp;lt;-
  mg %&amp;gt;%
  edit_model(model_name = &amp;quot;Logistic Regression PCA&amp;quot;,
             preProc = c(&amp;quot;nzv&amp;quot;, &amp;quot;center&amp;quot;, &amp;quot;scale&amp;quot;, &amp;quot;ICA&amp;quot;))
#&amp;gt; Model fit for Logistic Regression PCA has been deleted.

mg$models$`Logistic Regression PCA`
#&amp;gt; $method
#&amp;gt; [1] &amp;quot;glm&amp;quot;
#&amp;gt; 
#&amp;gt; $family
#&amp;gt; 
#&amp;gt; Family: binomial 
#&amp;gt; Link function: logit 
#&amp;gt; 
#&amp;gt; 
#&amp;gt; $preProc
#&amp;gt; [1] &amp;quot;nzv&amp;quot;    &amp;quot;center&amp;quot; &amp;quot;scale&amp;quot;  &amp;quot;ICA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you see, when you modify an existing model specification, any corresponding
fitted model is deleted, so that everything is in sync.&lt;/p&gt;
&lt;p&gt;You can also remove a model specification (including any fitted model) from
the model grid with the &lt;code&gt;remove_model&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(mg$models)
#&amp;gt; [1] &amp;quot;Funky Forest&amp;quot;                  &amp;quot;Logistic Regression Baseline&amp;quot; 
#&amp;gt; [3] &amp;quot;Logistic Regression PCA 98e-2&amp;quot; &amp;quot;Logistic Regression PCA&amp;quot;

# remove model configuration.
mg &amp;lt;-
  mg %&amp;gt;%
  remove_model(&amp;quot;Funky Forest&amp;quot;)
#&amp;gt; Model fit for Funky Forest has been deleted.

names(mg$models)
#&amp;gt; [1] &amp;quot;Logistic Regression Baseline&amp;quot;  &amp;quot;Logistic Regression PCA 98e-2&amp;quot;
#&amp;gt; [3] &amp;quot;Logistic Regression PCA&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s it! You should now be all set to grind models with the &lt;code&gt;modelgrid&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Peace out&lt;/p&gt;
&lt;p&gt;/smaakagen&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>